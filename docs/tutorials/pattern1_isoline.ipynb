{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35552911",
   "metadata": {},
   "source": [
    "# Pattern1 isoline demo (from Hugging Face dataset)\n",
    "\n",
    "This notebook is a **clean example** of how to run *pattern1 isoline* generation by importing functions from the `histoseg` package.\n",
    "\n",
    "Data source (Hugging Face dataset repo):\n",
    "- `cells.parquet`\n",
    "- `tissue_boundary.csv`\n",
    "- `analysis/clustering/gene_expression_graphclust/clusters.csv`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8007a311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install:\n",
    "# %pip install histoseg\n",
    "\n",
    "# If you are using Hugging Face dataset:\n",
    "# %pip install -U huggingface_hub pandas pyarrow numpy scipy scikit-learn seaborn matplotlib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaeffcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from histoseg.io.huggingface import download_xenium_outs\n",
    "\n",
    "repo_id = \"hutaobo/output-XETG00082_C105\"\n",
    "revision = \"main\"  # or a commit hash for full reproducibility\n",
    "\n",
    "paths = download_xenium_outs(repo_id, revision=revision)\n",
    "paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63f91bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Infer PATTERN1 from SFplot StructureMap (Searcher→Findee distances) ===\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display\n",
    "\n",
    "# 1) Import the two requested functions (plus the minimal helper needed to obtain cophenetic matrices)\n",
    "from histoseg.sfplot.Searcher_Findee_Score import (\n",
    "    compute_searcher_findee_distance_matrix_from_df,\n",
    "    compute_cophenetic_from_distance_matrix,   # in the same file; required to generate \"cophenetic\" matrices\n",
    "    plot_cophenetic_heatmap,\n",
    ")\n",
    "\n",
    "# 2) Build the per-cell dataframe required by compute_searcher_findee_distance_matrix_from_df\n",
    "#    Reuse the repo's alignment logic (Barcode ↔ cells.parquet id-like column + x/y inference).\n",
    "from histoseg.contours.pattern1_isoline import align_clusters_with_cells\n",
    "\n",
    "merged, id_col_used, x_col, y_col = align_clusters_with_cells(paths.clusters_csv, paths.cells_parquet)\n",
    "\n",
    "merged = merged.copy()\n",
    "merged[\"cluster\"] = pd.to_numeric(merged[\"cluster\"], errors=\"coerce\").astype(\"Int64\")\n",
    "merged = merged.dropna(subset=[\"cluster\"]).copy()\n",
    "merged[\"cluster\"] = merged[\"cluster\"].astype(int)\n",
    "\n",
    "print(f\"[OK] merged cells: {len(merged):,} | id_col_used='{id_col_used}' | x_col='{x_col}' | y_col='{y_col}'\")\n",
    "print(f\"[OK] n_clusters: {merged['cluster'].nunique()}\")\n",
    "\n",
    "# Optional speed knob for very large datasets (None = use all)\n",
    "MAX_CELLS = None  # e.g. 100_000\n",
    "if MAX_CELLS is not None and len(merged) > MAX_CELLS:\n",
    "    merged = merged.sample(n=MAX_CELLS, random_state=0).copy()\n",
    "    print(f\"[INFO] downsampled to {len(merged):,} cells for speed\")\n",
    "\n",
    "# 3) Compute directed mean 1-NN distance matrix: (searcher cluster) -> (findee cluster)\n",
    "distance_matrix = compute_searcher_findee_distance_matrix_from_df(\n",
    "    merged,\n",
    "    x_col=x_col,\n",
    "    y_col=y_col,\n",
    "    z_col=None,\n",
    "    celltype_col=\"cluster\",\n",
    ")\n",
    "display(distance_matrix.head())\n",
    "\n",
    "# 4) Convert to cophenetic distance matrices (normalized to [0, 1] inside the function)\n",
    "row_coph, col_coph = compute_cophenetic_from_distance_matrix(\n",
    "    distance_matrix,\n",
    "    method=\"average\",\n",
    "    show_corr=True,   # prints cophenetic correlation coefficients (row/col) for sanity check\n",
    ")\n",
    "\n",
    "# 5) Plot cophenetic heatmaps (return_figure=True to show inline, and to access dendrogram order)\n",
    "g_row = plot_cophenetic_heatmap(\n",
    "    row_coph,\n",
    "    matrix_name=\"row_coph\",\n",
    "    sample=repo_id.replace(\"/\", \"_\"),\n",
    "    return_figure=True,\n",
<<<<<<< HEAD
    ")"
=======
    ")\n",
>>>>>>> e2f42c620038595a4fbdef9e32d2d91914a0053a
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a28fad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Infer PATTERN1 from the StructureMap's clustered row order (Optional) ===\n",
    "# Heuristic: choose a contiguous block of size K with minimal mean intra-block cophenetic distance.\n",
    "K = 8  # the tutorial example uses 8 clusters; adjust if you want a different pattern size\n",
    "\n",
    "ordered_labels = [row_coph.index[i] for i in g_row.dendrogram_row.reordered_ind]\n",
    "row_coph_ord = row_coph.loc[ordered_labels, ordered_labels]\n",
    "\n",
    "def _mean_intrablock_distance(block_labels):\n",
    "    sub = row_coph_ord.loc[block_labels, block_labels].to_numpy()\n",
    "    iu = np.triu_indices_from(sub, k=1)\n",
    "    return float(sub[iu].mean()) if len(iu[0]) else 0.0\n",
    "\n",
    "best_block = None\n",
    "best_score = np.inf\n",
    "for start in range(0, len(ordered_labels) - K + 1):\n",
    "    block = ordered_labels[start : start + K]\n",
    "    score = _mean_intrablock_distance(block)\n",
    "    if score < best_score:\n",
    "        best_score = score\n",
    "        best_block = block\n",
    "\n",
    "# Convert labels to int if possible (GraphClust clusters are ints). Fall back to raw labels.\n",
    "try:\n",
    "    PATTERN1 = sorted(pd.to_numeric(pd.Index(best_block)).astype(int).tolist())\n",
    "except Exception:\n",
    "    PATTERN1 = list(best_block)\n",
    "\n",
    "print(f\"[RESULT] Inferred PATTERN1 (K={K}, mean_intra_distance={best_score:.4f}):\")\n",
    "print(PATTERN1)\n",
    "\n",
    "# Optional: show the selected submatrix for quick inspection\n",
    "display(row_coph.loc[best_block, best_block])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c13cd0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from histoseg.contours.pattern1_isoline import Pattern1IsolineConfig, run_pattern1_isoline\n",
    "\n",
    "# PATTERN1 is inferred in the previous cell (SFplot StructureMap).\n",
    "# If you want to override manually, uncomment and edit:\n",
    "# PATTERN1 = [10, 23, 19, 27, 14, 20, 25, 26]\n",
    "\n",
    "cfg = Pattern1IsolineConfig(\n",
    "    clusters_csv=paths.clusters_csv,\n",
    "    cells_parquet=paths.cells_parquet,\n",
    "    tissue_boundary_csv=paths.tissue_boundary_csv,\n",
    "    out_dir=\"outputs/pattern1_isoline0p5_from_graphclust\",\n",
    "    pattern1_clusters=PATTERN1,\n",
    "    grid_n=1200,\n",
    "    knn_k=30,\n",
    "    smooth_sigma=5,\n",
    "    min_cells_inside=10,\n",
    "    use_synth_bg=True,\n",
    "    save_params_json=False,\n",
    ")\n",
    "\n",
    "result = run_pattern1_isoline(cfg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45554e9",
   "metadata": {},
   "source": [
    "## Outputs\n",
    "\n",
    "The pipeline saves (by default):\n",
    "- `params.json` (all parameters + inferred join columns)\n",
    "- `pattern1_isoline_<level>_<i>.npy` (contour vertices)\n",
    "- `pattern1_isoline_<level>.png` (quick preview)\n",
    "\n",
    "Check `result.out_dir` for the exact location."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
